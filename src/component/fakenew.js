const blogsData = [{
    id: "0",
    img: "https://i.cbc.ca/1.5030390.1550866748!/fileImage/httpImage/image.jpg_gen/derivatives/16x9_780/403937413-robot-typing-ai-writing.jpg",
    name: "The true dangers of AI are closer than we think",
    date: "Oct 21, 2020, 5:58 AM",
    author: "Karen Hao",
    summary: "As long as humans have built machines, we’ve feared the day they could destroy us. Stephen Hawking famously warned that AI could spell an end to civilization. But to many AI researchers, these conversations feel unmoored. It’s not that they don’t fear AI running amok—it’s that they see it already happening, just not in the ways most people would expect.",
    blogUrl: "https://media.npr.org/assets/img/2016/11/23/gettyimages-494330351-2_custom-ba3ef5a85ff4b5e31d0447b0dd8e17499cc0e4c3-s1600-c85.jpg",
    description: "As long as humans have built machines, we’ve feared the day they could destroy us. Stephen Hawking famously warned that AI could spell an end to civilization. But to many AI researchers, these conversations feel unmoored. It’s not that they don’t fear AI running amok—it’s that they see it already happening, just not in the ways most people would expect. AI is now screening job candidates, diagnosing disease, and identifying criminal suspects. But instead of making these decisions more efficient or fair, it’s often perpetuating the biases of the humans on whose decisions it was trained. William Isaac is a senior research scientist on the ethics and society team at DeepMind, an AI startup that Google acquired in 2014. He also cochairs the Fairness, Accountability, and Transparency conference—the premier annual gathering of AI experts, social scientists, and lawyers working in this area. I asked him about the current and potential challenges facing AI development—as well as the solutions."
},
{
    id: "1",
    img: "https://media.wired.com/photos/5c6498b5b675045db13fd243/master/w_2560%2Cc_limit/OpenAI_FakeNews_Top-Art.png",
    name: "A huge fake-nude Telegram operation shows the big threat posed by deepfakes is revenge porn, not disinformation",
    date: "Oct 21, 2020, 7:58 AM",
    author: "Isobel Asher Hamilton",
    summary: "A chilling investigation into a bot service that generates fake nudes has highlighted that the most urgent danger internet 'deepfakes' pose isn't disinformation — it's revenge porn.",
    blogUrl: "https://images.thequint.com/thequint%2F2020-10%2F64796f47-af17-4ab1-8431-686199ff36b9%2FiStock_880349732__1_.jpg?rect=0%2C0%2C1732%2C974&auto=format%2Ccompress&fmt=webp&w=480&dpr=2.0",
    description: "A chilling investigation into a bot service that generates fake nudes has highlighted that the most urgent danger internet 'deepfakes' pose isn't disinformation — it's revenge porn.Deepfake-monitoring firm Sensity, previously Deeptrace, on Tuesday revealed it had discovered a huge operation disseminating AI-generated nude images of women and, in some cases, underage girls. The service was operating primarily on the encrypted messaging app Telegram using an AI-powered bot.  Users would send an image of a woman they wanted to see naked to the bot. The bot would then 'strip-naked' the image, generating a fake nude body, and superimpose it onto the original image of the woman in question."
},
{
    id: "2",
    img: "https://media3.s-nbcnews.com/j/newscms/2020_38/3412853/200917-data-stock-mn-1600_720d7a0980daa013126ca4e519c9607f.fit-2000w.jpg",
    name: "Have you read something written by GPT-3? Probably not, but it's hard to be sure",
    date: "Oct 21, 2020, 7:58 AM",
    author: "Melanie Ehrenkranz",
    summary: "A powerful language generator released on the internet has renewed concerns about how such tools can be used and the biases they can inherit.",
    blogUrl: "http://fundevelopers.tech/wp-content/uploads/2020/03/555.jpg",
    description: "In August, a blog post about personal productivity surfaced at the top of Hacker News, a website and message board well known in Silicon Valley circles that focuses on computer science and entrepreneurship.  There was nothing terribly unique about the post. It offered simple, relatively common advice. Some readers discussed its merits in comments and offered their own thoughts about how to be more productive. A few people, however, found the post a little suspicious.'This is either something written by GPT-3, or the human equivalent,' one person commented. 'Zero substantive content, pure regurgitation.' Another person had the same assessment: 'I think this was written by GPT-3.'It turns out that they were right. The blog post was written almost entirely by software called Generative Pre-trained Transformer 3, or GPT-3. Liam Porr, a computer science student at the University of California, Berkeley, used the new machine learning model to generate the post with the intention of fooling the public into believing it was the product of a human mind.  He wanted to see just how good GPT-3 is.'With something like a language model, there's not a good quantitative way to see how good it is, because language and writing is qualitative,'Porr said. 'With this kind of experiment, I can concretely say 20,000 unique people came to my website and only three actually had the sense to say it was written by a robot.' GPT-3 isn't the first natural language program of its kind, but it has already gotten widespread attention for how good it is at mimicking simple human writing. But its release into the world, while not entirely public, has caused some concern that it could be used to quickly and cheaply generate misinformation or propaganda. While it was a harmless experiment, Porr's post offered a concrete example of the risk. And it adds GPT-3 to other advanced software that has been disseminated through the internet and has caused alarm. Deepfake technology, which can make doctored videos of people, has become common enough to spur congressional hearings.."
},
{
    id: "3",
    img: "https://scx2.b-cdn.net/gfx/news/2020/5eecd64cb4e87.jpg",
    name: "OpenAI releases powerful text generator",
    date: "JUNE 19, 2020 ",
    author: "Eve Kessler ",
    summary: "Artificial intelligence laboratory OpenAI announced it is making a powerful new neural network for natural language processing available for limited release to the public.",
    blogUrl: "https://techcrunch.com/wp-content/uploads/2019/02/GettyImages-1009465514.jpg?w=1390&crop=1",
    description: "The laboratory, founded by Elon Musk and recently supported by a $1 billion grant from Microsoft, has designed text generators that create readable passages virtually indistinguishable from those written by humans. OpenAI's machine learning approach scrapes massive amounts of data from the web and analyzes it for statistical patterns that allow it to realistically predict what letters or words will likely be written next. When users feed a word or phrase or longer text snippets into the generator, it expands on the words with convincingly humanlike text. The results can be used to create stories, tackle reading comprehension exercises, answer questions, summarize theses or even play chess, solve mathematical problems or create text-based Dungeon scenarios. Dubbed GPT-3, the text generator relies on a huge database composed of nearly a trillion words amassed from scans of web posts and digital books. Microsoft built a supercomputer stocked with hundreds of thousands of processors for the project.The program is proficient at creating factual passages and works of fiction. An early version of the text generator, in fact, was so good at creating original text that it raised concerns among its creators that it could be used for nefarious purposes such as spreading false news stories on the web or engaging in schemes to cheat consumers through fake online chats."
},
{
    id: "4",
    img: "https://img.etimg.com/thumb/msid-72895486,width-300,imgsize-558250,,resizemode-4,quality-100/12.jpg",
    name: "Facebook is using AI to stem dangerous and false posts",
    date: "NOVEMBER 14, 2020",
    author: "Peter Grad",
    summary: "Facebook has come under withering criticism this past year from folks who say the company is not doing enough to stem hate speech, online harassment and the spread of false news stories. ",
    blogUrl: "https://festadelluvaimpruneta.it/wp-content/uploads/2017/11/tech-news-healthcare-iot-mobile-smart-cities2.jpg",
    description: "Facebook has come under withering criticism this past year from folks who say the company is not doing enough to stem hate speech, online harassment and the spread of false news stories.To be fair, the task of policing the activities of 1.62 billion daily users generating 4 petabytes of data, including 350 million photos, per day is no small task. It's not easy being the world's largest social platform.  Still, the company has been criticized for allowing scores of hate-fueled groups to spread offensive and threatening posts and for allowing ultra-rightwing conspiracy-theory groups such as QAnon to freely spread false political allegations. Academic and governmental analyses of the 2016 presidential election uncovered evidence of massive interference by domestic and foreign actors, and it appears similar efforts were undertaken in the 2020 election as well.  Facebook employs 15,000 content moderators to review reports of misbehavior ranging from political subterfuge to harassment to terroristic threats to child exploitation. They have generally tackled reports chronologically, frequently allowing more serious allegations to go unaddressed for days while lesser issues were reviewed. On Friday, Facebook announced that it will bring machine learning into he moderating process. It will utilize algorithms to detect the most severe issues and assign them to human moderators. Software moderators will continue to handle lower-level abuse such as copyright infringement and spam."
    
    
}
]


export default blogsData